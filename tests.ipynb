{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sizes = [784, 30, 10]\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def cost_derivative(output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "def backprop(x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(biases, weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, 3):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "def update_mini_batch(mini_batch=10, beta_1=0.9, beta_2=0.999, t=0):\n",
    "            nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "            nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "            # Llamo a las variables m y v que están fuera de esta función y las actualizo\n",
    "            m_b = [abs(np.zeros(b.shape)) for b in biases] # m para las b\n",
    "            v_b = [abs(np.zeros(b.shape)) for b in biases] # v para las b\n",
    "            m_w = [abs(np.zeros(w.shape)) for w in weights] # m para las w\n",
    "            v_w = [abs(np.zeros(w.shape)) for w in weights] # v para las w\n",
    "            for x, y in mini_batch:\n",
    "                delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
    "                nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "                nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "            m_b = [beta_1*mb + (1-beta_1)*nb for mb, nb in zip(m_b, nabla_b)]\n",
    "            v_b = [beta_2*vb + (1-beta_2)*(nb**2) for vb, nb in zip(v_b, nabla_b)]\n",
    "            m_w = [beta_1*mw + (1-beta_1)*nw for mw, nw in zip(m_w, nabla_w)]\n",
    "            v_w = [beta_2*vw + (1-beta_2)*(nw**2) for vw, nw in zip(v_w, nabla_w)]\n",
    "            \n",
    "            # creo las hats de cada m y v\n",
    "            m_b_hat = [mb/(1-beta_1*t) for mb in m_b]\n",
    "            m_w_hat = [mw/(1-beta_1*t) for mw in m_w]\n",
    "            v_b_hat = [vb/(1-beta_2*t) for vb in v_b]\n",
    "            v_w_hat = [vw/(1-beta_2*t) for vw in v_w]\n",
    "            \n",
    "            return (m_b_hat, m_w_hat, v_b_hat, v_w_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Desktop\\Uni\\6th Semester\\Redes Neuronales\\Red-Neuronal\\tests.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m_b_hat, m_w_hat, v_b_hat, v_w_hat \u001b[39m=\u001b[39m update_mini_batch()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m v_b_hat:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\USER\\Desktop\\Uni\\6th Semester\\Redes Neuronales\\Red-Neuronal\\tests.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m m_w \u001b[39m=\u001b[39m [\u001b[39mabs\u001b[39m(np\u001b[39m.\u001b[39mzeros(w\u001b[39m.\u001b[39mshape)) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m weights] \u001b[39m# m para las w\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m v_w \u001b[39m=\u001b[39m [\u001b[39mabs\u001b[39m(np\u001b[39m.\u001b[39mzeros(w\u001b[39m.\u001b[39mshape)) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m weights] \u001b[39m# v para las w\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m mini_batch:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     delta_nabla_b, delta_nabla_w \u001b[39m=\u001b[39m backprop(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Uni/6th%20Semester/Redes%20Neuronales/Red-Neuronal/tests.ipynb#W5sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     nabla_b \u001b[39m=\u001b[39m [nb\u001b[39m+\u001b[39mdnb \u001b[39mfor\u001b[39;00m nb, dnb \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(nabla_b, delta_nabla_b)]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "m_b_hat, m_w_hat, v_b_hat, v_w_hat = update_mini_batch()\n",
    "        \n",
    "for i in v_b_hat:\n",
    "    if i < 0:\n",
    "        print(i)\n",
    "        \n",
    "for i in v_w_hat:\n",
    "    if i < 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
